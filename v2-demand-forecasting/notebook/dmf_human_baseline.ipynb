{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dmf-human-baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVG6qIgtanCu"
      },
      "source": [
        "# Demand Forecasting (AI, Data Innovation Team, CJ Express, TILDI)\n",
        "\n",
        "## File: Human Baseline Model\n",
        "\n",
        " C.J. Express Group Co.,Ltd. All Rights Reserved.\n",
        "  \n",
        " Year: 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPwz-mEbcXIx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_gX9lyyR-5p",
        "outputId": "dfe1a27c-8d1e-4a11-986b-ee469b056097"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(spark)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f6e9c671e50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFuAaQ8LnIIY"
      },
      "source": [
        "# import library\n",
        "from google.cloud import bigquery\n",
        "from datetime import date, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from graphframes import *\n",
        "from itertools import combinations\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "\n",
        "from pyspark.sql import Row, SparkSession, functions as F\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
        "\n",
        "import math\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDvUvjbaHXUV"
      },
      "source": [
        "# Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgjOmhqQHYjH"
      },
      "source": [
        "def extract_date(df_extract_date, columns='SalDate'):\n",
        "    \"\"\"Extract data to date features.\n",
        "\n",
        "    :param spark: Spark session object.\n",
        "    :return: Spark DataFrame.\n",
        "    \"\"\"\n",
        "    split_col = F.split(df_extract_date[columns], '-')\n",
        "    df_extract_date = (df_extract_date\n",
        "                        .withColumn('DayofMonth', split_col.getItem(2).cast('integer')))\n",
        "    df_extract_date = (df_extract_date\n",
        "                        .withColumn('Yearday', F.dayofyear(F.col(columns)))\n",
        "                        .withColumn('Month', F.month(F.col(columns)))\n",
        "                        .withColumn('DayofWeek', F.dayofweek(F.col(columns)))\n",
        "                        .withColumn('Year', F.year(F.col(columns)))\n",
        "                        .withColumn('Quarter', F.quarter(F.col(columns)))\n",
        "                        .withColumn('WeekOfYear', F.weekofyear(F.col(columns))))\n",
        "      \n",
        "    return df_extract_date\n",
        "\n",
        "def stratified_split_train_test(df, frac, label, join_on, seed=42):\n",
        "    fractions = df.select(label).distinct().withColumn(\"fraction\", F.lit(frac)).rdd.collectAsMap()\n",
        "    df_frac = df.stat.sampleBy(label, fractions, seed)\n",
        "    df_remaining = df.join(df_frac, on=join_on, how=\"left_anti\")\n",
        "    return df_frac, df_remaining\n",
        "\n",
        "def cast_double_types(df_cast, c=[]):\n",
        "  for col in c:\n",
        "    df_cast = df_cast.withColumn(\n",
        "      col,\n",
        "      F.col(col).cast(\"double\")\n",
        "    )\n",
        "  return df_cast\n",
        "\n",
        "def cast_int_types(df_cast, c=[]):\n",
        "  for col in c:\n",
        "    df_cast = df_cast.withColumn(\n",
        "      col,\n",
        "      F.col(col).cast(\"int\")\n",
        "    )\n",
        "  return df_cast\n",
        "\n",
        "def cast_date_types(df_cast, c=[]):\n",
        "  for col in c:\n",
        "    df_cast = df_cast.withColumn(\n",
        "      col,\n",
        "      F.col(col).cast(DateType())\n",
        "    )\n",
        "  return df_cast\n",
        "\n",
        "def percentage_error(actual, predicted):\n",
        "    res = np.empty(actual.shape)\n",
        "    for j in range(actual.shape[0]):\n",
        "        if actual[j] != 0:\n",
        "            res[j] = np.absolute(actual[j] - predicted[j]) / actual[j]\n",
        "        else:\n",
        "            res[j] = predicted[j] / np.mean(actual)\n",
        "    return res\n",
        "\n",
        "def mape(y_true, y_pred): \n",
        "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n",
        "\n",
        "def rmsle_spark(df, label = 'TotalQtySale'):\n",
        "    sle = ((F.log(F.col('prediction') + F.lit(1)) - F.log(F.col(label) + F.lit(1)))**2).alias('sle')\n",
        "    df = df.select(sle)\n",
        "    return np.sqrt(df.groupby().agg(F.avg('sle').alias('msle')).collect()[0][0])\n",
        "\n",
        "def mape_spark(df, label = 'TotalQtySale'):\n",
        "    ape = ((F.abs(F.col('prediction') - F.col(label)) * F.lit(100)) / F.col(label)).alias('ape')\n",
        "    df = df.select(ape)\n",
        "    return df.groupby().agg(F.avg('ape').alias('mape')).collect()[0][0]\n",
        "  \n",
        "def smape_spark(df, label = 'TotalQtySale'):\n",
        "    sape = ((F.abs(F.col('prediction') - F.col(label)) * F.lit(100))\\\n",
        "           / ((F.col('prediction') + F.col(label)) / F.lit(2))).alias('sape')\n",
        "    df = df.select(sape)\n",
        "    return df.groupby().agg(F.avg('sape').alias('smape')).collect()[0][0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN4c3qjwHJWn"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crnUaznuWnaD",
        "outputId": "2067b84f-3a30-45dd-bf50-c0ecf2dd9ea2"
      },
      "source": [
        "# define features\n",
        "label_cols = ['TotalQtySale']\n",
        "numeric_features = ['avgPriceDis','avgPrice','Yearday','Month',\n",
        "                    'DayofWeek','Year','Quarter','WeekOfYear','DayofMonth']\n",
        "string_features = ['BranchCode','MaterialCode','types'] # types = promotions\n",
        "\n",
        "# initiate variables\n",
        "select_variables_initial = ['BranchCode','MaterialCode','Date','avgPriceDis',\n",
        "                            'avgPrice','supPrice','label','totalNetSale',\n",
        "                            'TotalQtySale','types','Branch','Name']\n",
        "\n",
        "\n",
        "# define the model name\n",
        "version = 'BASE_MODEL_001'\n",
        "versionFeaturePL = \"BASE_PL_MODEL_001\"\n",
        "\n",
        "Pipeline_Model_PATH = \"model_pipeline_\"+str(versionFeaturePL)+\".plmodel\"\n",
        "Model_PATH = \"model_ml_\"+str(version)+\".plmodel\"\n",
        "\n",
        "# input\n",
        "data = './tildi_demandforecasting_type1_poc.csv'\n",
        "data_location = './location.csv'\n",
        "data_category = './category.csv'\n",
        "\n",
        "# training/testing split date\n",
        "split_date = '2020-12-01'\n",
        "\n",
        "\n",
        "# output\n",
        "result_list = []\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main ML script definition.\n",
        "\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # # start Spark application and get Spark session, logger and config\n",
        "    # spark, log, config = start_spark(\n",
        "    #     app_name='my_ml_job',\n",
        "    #     files=['configs/ml_config.json'])\n",
        "\n",
        "    # # log that main ETL job is starting\n",
        "    # log.warn('cj_ml_job is up-and-running')\n",
        "\n",
        "    # import data \n",
        "    df = (spark\n",
        "          .read \n",
        "          .format(\"csv\")\n",
        "          .option(\"header\", \"true\")\n",
        "          .load(data))\n",
        "\n",
        "    df_location = (spark\n",
        "                  .read.format(\"csv\")\n",
        "                  .option(\"header\", \"true\")\n",
        "                  .load(data_location))\n",
        "\n",
        "    df_category = (spark\n",
        "                  .read.format(\"csv\")\n",
        "                  .option(\"header\", \"true\")\n",
        "                  .load(data_category))\n",
        "\n",
        "    # cache data\n",
        "    df.cache()\n",
        "    df_location.cache()\n",
        "    df_category.cache()\n",
        "\n",
        "    # initial variables\n",
        "    df = df.select(select_variables_initial)\n",
        "\n",
        "    # join location\n",
        "    df = df.join(df_location, how='left',on =['BranchCode'] )\n",
        "\n",
        "    # clean\n",
        "    df = df.dropna(how='all')\n",
        "\n",
        "    # casting type\n",
        "    df = cast_double_types(df, ['avgPriceDis','avgPrice','TotalQtySale','supPrice','label','totalNetSale',])\n",
        "    df = cast_int_types(df, ['label','ZipCode'])\n",
        "    df = cast_date_types(df, ['Date'])\n",
        "\n",
        "    # extracting date feature\n",
        "    df = extract_date(df,'Date')\n",
        "\n",
        "    # clean\n",
        "    df2 = df.dropna()\n",
        "    # check missing value\n",
        "    # df2.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df2.columns)).show()\n",
        "\n",
        "    # Train/Test Split\n",
        "    df_train = (df\n",
        "                .filter(df[\"Date\"]<split_date)) \n",
        "    df_test = (df\n",
        "                .filter((df[\"Date\"]>=split_date)))\n",
        "\n",
        "    # creating new features: item average as prediction\n",
        "    df_train_avg = (df_train.groupBy(\"BranchCode\", \"MaterialCode\")\n",
        "                           .agg(avg(\"TotalQtySale\")\n",
        "                           .alias('prediction_avg')))\n",
        "    df_train_avg_dow = (df_train.groupBy(\"BranchCode\", \"MaterialCode\", \"DayofWeek\")\n",
        "                              .agg(avg(\"TotalQtySale\")\n",
        "                               .alias('prediction_avg_dow')))\n",
        "    df_train_avg_by_month = (df_train.groupBy(\"BranchCode\", \"MaterialCode\", \"Month\")\n",
        "                                    .agg(avg(\"TotalQtySale\")\n",
        "                                    .alias('prediction_avg_month')))\n",
        "    df_train_avg_all_store = (df_train.groupBy(\"MaterialCode\")\n",
        "                                      .agg(avg(\"TotalQtySale\")\n",
        "                                      .alias('prediction_avg_all_store')))\n",
        "    df_train_avg_dow_all_store = (df_train.groupBy(\"MaterialCode\", \"DayofWeek\")\n",
        "                                          .agg(avg(\"TotalQtySale\")\n",
        "                                          .alias('prediction_avg_dow_all_store')))\n",
        "    df_train_avg_by_month_all_store = (df_train.groupBy(\"MaterialCode\", \"Month\")\n",
        "                                              .agg(avg(\"TotalQtySale\")\n",
        "                                              .alias('prediction_avg_month_all_store')))\n",
        "\n",
        "    # join new features\n",
        "    df_test2 = (df_test\n",
        "            .join(df_train_avg_all_store, \n",
        "                  how='left', \n",
        "                  on =['MaterialCode']))\n",
        "    df_test2 = (df_test2\n",
        "                .join(df_train_avg_dow_all_store, \n",
        "                      how='left', \n",
        "                      on =['MaterialCode','DayofWeek']))\n",
        "    \n",
        "    # # remove missing value\n",
        "    df_test2 = df_test2.dropna()\n",
        "\n",
        "    # # # check missing value\n",
        "    # print(df_test2.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_test2.columns))\n",
        "    #     .show())\n",
        "\n",
        "    df_predict = (df_test2.withColumn(\"prediction\", \n",
        "                                      df_test2[\"prediction_avg_all_store\"])) # prediction_avg_all_store, prediction_avg_dow_all_store \n",
        "\n",
        "    # Performance Evaluation: Overall\n",
        "    actual = list(df_predict.select('TotalQtySale')\n",
        "                            .toPandas()['TotalQtySale'])\n",
        "    pred = list(df_predict.select('prediction')\n",
        "                            .toPandas()['prediction'])\n",
        "\n",
        "    #for metricName in ['rmse','mse','r2','mae']:# all metrics\n",
        "    for metricName in ['rmse','mae']:\n",
        "        evaluator = RegressionEvaluator(labelCol=\"TotalQtySale\", predictionCol=\"prediction\", metricName=metricName)\n",
        "        result = evaluator.evaluate(df_predict)\n",
        "        print ('%s = %g' % (metricName,result))\n",
        "        result_list.append(result)\n",
        "\n",
        "    # mape\n",
        "    mape = mape_spark(df_predict, label = 'TotalQtySale')\n",
        "    result_list.append(mape)\n",
        "    print(\"mape =\", mape)\n",
        "\n",
        "    # rmsle\n",
        "    rmsle = rmsle_spark(df_predict, label = 'TotalQtySale')\n",
        "    result_list.append(rmsle)\n",
        "    print(\"rmsle =\", rmsle)\n",
        "\n",
        "    # accuracy\n",
        "    acc = (1-(np.exp(rmsle)-1))*100\n",
        "    result_list.append(acc)\n",
        "    print(\"acc =\", acc)\n",
        "\n",
        "    print(result_list)\n",
        "    print(\"Inference Process: Done\")\n",
        "\n",
        "    # Performance Evaluation: OUT OF STOCK (OOS)\n",
        "    overall_test_number = df_predict.count()\n",
        "    oos_number = (df_predict\n",
        "                .filter((df_predict[\"prediction\"]<df_predict[\"TotalQtySale\"])\n",
        "                    &(df_predict[\"TotalQtySale\"]!=0)).count())\n",
        "    print(\"percent of oos\", oos_number*100/overall_test_number) \n",
        "\n",
        "    # log the success and terminate Spark application\n",
        "    # log.warn('cj_ml_job is finished')\n",
        "    \n",
        "    #spark.stop()\n",
        "    \n",
        "    return None\n",
        "\n",
        "# entry point for CJ PySpark ML application\n",
        "if __name__ == '__main__':\n",
        "    # start time\n",
        "    start_time = time.time()\n",
        "    main()\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rmse = 3.46269\n",
            "mae = 0.622755\n",
            "mape = 77.8636734280258\n",
            "rmsle = 0.4317214181538035\n",
            "acc = 46.00939344735213\n",
            "[3.462687450866297, 0.6227547545467854, 77.8636734280258, 0.4317214181538035, 46.00939344735213]\n",
            "Inference Process: Done\n",
            "percent of oos 15.433566951024853\n",
            "--- 132.91115593910217 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}